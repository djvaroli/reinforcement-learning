{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Callable, Any\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from jax import numpy as jnp\n",
    "from jax import random as jrnd\n",
    "from jax import Array\n",
    "\n",
    "from action.selection import GreeedyActionSelection\n",
    "from bandit import KArmedBandit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1000\n",
    "random_key = jrnd.PRNGKey(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_arms = 10\n",
    "init_q = 0\n",
    "\n",
    "n_steps = 100\n",
    "n_experiments = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_selection = GreeedyActionSelection()\n",
    "\n",
    "bandit = KArmedBandit(\n",
    "    n_arms, \n",
    "    action_selection,\n",
    "    init_q, \n",
    "    SEED\n",
    ")\n",
    "\n",
    "reward_history = jnp.zeros((n_experiments, n_steps))\n",
    "selected_action_history = jnp.zeros((n_experiments, n_steps), dtype=int)\n",
    "optimal_action_history = jnp.zeros((n_experiments,), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(range(n_experiments), total=n_experiments) as pbar:\n",
    "    for experiment in pbar:\n",
    "        pbar.set_description(f\"Run {experiment + 1}/{n_experiments}\")\n",
    "\n",
    "        # initialize the bandit. Automatically updates the random key such that\n",
    "        # the next run will have a different starting random key\n",
    "        bandit.reinit()\n",
    "        optimal_action_history = optimal_action_history.at[experiment].set(bandit.optimal_action)\n",
    "        \n",
    "        for t in range(n_steps):\n",
    "            reward, action = bandit.pull()\n",
    "            \n",
    "            reward_history = reward_history.at[experiment, t].set(reward)\n",
    "            selected_action_history = selected_action_history.at[experiment, t].set(action)\n",
    "\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward = reward_history.mean(axis=0)\n",
    "reward_std = reward_history.std(axis=0)\n",
    "\n",
    "# plot the mean reward and a shaded region representing the standard deviation\n",
    "plt.plot(mean_reward)\n",
    "plt.fill_between(\n",
    "    range(n_steps), mean_reward - reward_std, mean_reward + reward_std, alpha=0.2\n",
    ")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Average reward\")\n",
    "\n",
    "# save the plot\n",
    "plt.savefig(\"bandit_reward.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "was_optimal_policy_selected = selected_action_history == optimal_action_history.reshape(-1, 1)\n",
    "optimal_policy_rate = was_optimal_policy_selected.mean(axis=0)\n",
    "optimal_policy_rate_std = was_optimal_policy_selected.std(axis=0)\n",
    "\n",
    "# plot the mean reward and a shaded region representing the standard deviation\n",
    "plt.plot(optimal_policy_rate)\n",
    "plt.fill_between(\n",
    "    range(n_steps), \n",
    "    optimal_policy_rate - optimal_policy_rate_std, \n",
    "    optimal_policy_rate + optimal_policy_rate_std,\n",
    "    alpha=0.2\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"% Optimal action\")\n",
    "plt.legend()\n",
    "\n",
    "# save the plot\n",
    "plt.savefig(\"bandit_optimal_action.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-demos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
